{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyM3chHU1TI99U75M5/G7beD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Train a CNN to make predictions based on specific train and validation sets"],"metadata":{"id":"smPB1gwQtXCQ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vjx60ntBha1W","executionInfo":{"status":"ok","timestamp":1729918071468,"user_tz":-180,"elapsed":52561,"user":{"displayName":"Konstantinos Roumeliotis","userId":"17264923090131634662"}},"outputId":"e646f7b0-8e00-4940-e41e-4d72413035d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 9111924967842897683\n","xla_global_id: -1\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 21991653376\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 14447801152493232884\n","physical_device_desc: \"device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\"\n","xla_global_id: 416903419\n","]\n","Mounted at /content/gdrive\n","Epoch 1/3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m534/534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.4880 - loss: 0.6925 - val_accuracy: 0.5600 - val_loss: 0.6848\n","Epoch 2/3\n","\u001b[1m534/534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6397 - loss: 0.6780 - val_accuracy: 0.6325 - val_loss: 0.6575\n","Epoch 3/3\n","\u001b[1m534/534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6881 - loss: 0.6395 - val_accuracy: 0.8750 - val_loss: 0.5884\n","Training Loss: [0.6903658509254456, 0.6711488962173462, 0.6252743601799011]\n","Validation Loss: [0.6847872734069824, 0.6575319766998291, 0.5884484648704529]\n","Validation Accuracy: [0.5600000023841858, 0.6324999928474426, 0.875]\n","Training time: 47.90 seconds\n"]}],"source":["import pandas as pd # For data manipulation\n","from tensorflow.keras.preprocessing.text import Tokenizer # For tokenizing text\n","from tensorflow.keras.optimizers import Adam # Adam optimizer for training\n","from tensorflow.keras.preprocessing.sequence import pad_sequences # For padding sequences\n","from tensorflow.keras.models import Sequential # Sequential model for building the neural network\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense # Layers for the neural network\n","from tensorflow.python.client import device_lib # A library for checking available devices in TensorFlow\n","import time\n","from google.colab import drive\n","import os\n","\n","# Check available devices\n","devices = device_lib.list_local_devices()\n","print(devices)\n","\n","class CNNTraining:\n","    def __init__(self, learning_rate, epochs, batch_size, max_len, feature_col, label_col):\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.max_len = max_len\n","        self.history = None\n","\n","    def load_data(self, train_file_path, val_file_path):\n","        drive.mount('/content/gdrive') # Mount Google Drive to access files\n","        train_df = pd.read_csv(train_file_path)\n","        val_df = pd.read_csv(val_file_path)\n","\n","        # Extracting text and labels from training data\n","        self.train_texts = train_df[feature_col].tolist()\n","        self.train_labels = train_df[label_col].values\n","\n","        # Extracting text and labels from validation data\n","        self.val_texts = val_df[feature_col].tolist()\n","        self.val_labels = val_df[label_col].values\n","\n","    def preprocess_data(self):\n","        self.tokenizer = Tokenizer() # Initializing Tokenizer\n","        self.tokenizer.fit_on_texts(self.train_texts) # Fitting tokenizer on training text data\n","\n","        # Converting text data to sequences\n","        train_sequences = self.tokenizer.texts_to_sequences(self.train_texts)\n","        val_sequences = self.tokenizer.texts_to_sequences(self.val_texts)\n","\n","        # Padding sequences to a fixed length\n","        self.train_data = pad_sequences(train_sequences, maxlen=self.max_len, padding='post')\n","        self.val_data = pad_sequences(val_sequences, maxlen=self.max_len, padding='post')\n","\n","    def build_model(self):\n","        self.model = Sequential() # Initializing sequential model\n","        self.model.add(Embedding(len(self.tokenizer.word_index) + 1, 128, input_length=self.max_len)) # Adding Embedding layer\n","        self.model.add(Conv1D(128, 5, activation='relu')) # Adding Convolutional layer\n","        self.model.add(GlobalMaxPooling1D()) # Adding GlobalMaxPooling layer\n","        self.model.add(Dense(64, activation='relu')) # Adding Dense layer\n","        self.model.add(Dense(1, activation='sigmoid')) # Adding Output layer\n","\n","        optimizer = Adam(learning_rate=self.learning_rate) # Initializing Adam optimizer\n","        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) # Compiling model\n","\n","    def train_model(self):\n","        self.history = self.model.fit(self.train_data, self.train_labels, epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.val_data, self.val_labels)) # Training the model\n","\n","    def get_training_loss(self):\n","        return self.history.history['loss']\n","\n","    def get_validation_loss(self):\n","        return self.history.history['val_loss']\n","\n","    def get_validation_accuracy(self):\n","        return self.history.history['val_accuracy']\n","\n","    def save_model(self, save_dir, model_name):\n","      os.makedirs(save_dir, exist_ok=True) # Creating directory if not exists\n","      self.model.save(os.path.join(save_dir, model_name + '.keras')) # Saving the model with .keras extension\n","\n","# Usage:\n","start_time = time.time()\n","model = 'cnn'\n","\n","## Hyperparameters\n","learning_rate = 2e-5\n","epochs = 3\n","batch_size = 6\n","max_len = 4096\n","optimizer = 'Adam' # TODO! Need to create the functionality to switch optimizers between Adam and AdamW\n","\n","## Paths and filenames\n","absolute_path = '/content/gdrive/My Drive/Projects/SpamNews/'\n","train_file_path = 'Datasets/train_set.csv'\n","val_file_path = 'Datasets/validation_set.csv'\n","save_dir = 'TrainedModels/'\n","trained_model = model + '_optimizer_' + optimizer + '_lr_' + str(learning_rate) + '_epochs_' + str(epochs) + '_bs_' + str(batch_size) + '_maxlen_' + str(max_len)\n","feature_col = 'Text'\n","label_col = 'Label'\n","\n","trainer = CNNTraining(learning_rate, epochs, batch_size, max_len, feature_col, label_col) # Creating instance of CNNTraining class\n","trainer.load_data(absolute_path + train_file_path, absolute_path + val_file_path) # Loading data\n","trainer.preprocess_data() # Preprocessing data\n","trainer.build_model() # Building model\n","trainer.train_model() # Training model\n","trainer.save_model(absolute_path + save_dir, trained_model) # Saving trained model\n","\n","print(\"Training Loss:\", trainer.get_training_loss())\n","print(\"Validation Loss:\", trainer.get_validation_loss())\n","print(\"Validation Accuracy:\", trainer.get_validation_accuracy())\n","print(\"Training time: {:.2f} seconds\".format(time.time() - start_time))"]},{"cell_type":"markdown","source":["## Use the trained CNN model to make predictions for a specific test set"],"metadata":{"id":"Vn5tJPMhstpd"}},{"cell_type":"code","source":["import pandas as pd\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import os\n","from google.colab import drive\n","\n","class CNNPredictions:\n","    def __init__(self, max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col):\n","        self.max_len = max_len\n","        self.test_file_path = test_file_path\n","        self.predictions_path = predictions_path\n","        self.absolute_path = absolute_path\n","        self.trained_model = trained_model\n","        self.feature_col = feature_col\n","        self.prediction_col = prediction_col\n","        drive.mount('/content/gdrive') # Mount Google Drive to access files\n","\n","    def predict(self):\n","      # Load test dataset\n","      test_df = pd.read_csv(self.absolute_path + self.test_file_path)\n","      test_texts = test_df[self.feature_col].tolist()  # Extract the text data from the specified feature column\n","\n","      # Tokenize text data using the same tokenizer used during training\n","      tokenizer = Tokenizer()  # Initialize a Tokenizer object\n","      tokenizer.fit_on_texts(test_texts)  # Fit the tokenizer on the test text data\n","      test_sequences = tokenizer.texts_to_sequences(test_texts)  # Convert text data to sequences of integers\n","\n","      test_data = pad_sequences(test_sequences, maxlen=self.max_len, padding='post')  # Pad sequences to the maximum length specified during training\n","\n","      saved_model = load_model(self.absolute_path + self.trained_model)  # Load the trained model from the specified path\n","\n","      # Make predictions on test data using the loaded model\n","      predictions = saved_model.predict(test_data)  # Use the loaded model to make predictions on the test data\n","\n","      # Convert predictions to binary labels (0 or 1) based on a threshold (e.g., 0.5)\n","      binary_predictions = (predictions > 0.5).astype(int)\n","\n","      # Add the binary predictions as a new column to the test dataframe\n","      test_df[self.prediction_col] = binary_predictions\n","\n","      # Save predictions to CSV\n","      test_df.to_csv(self.absolute_path + self.predictions_path, index=False)\n","\n","      print(\"Predictions done\")\n","\n","# Usage:\n","max_len = 4096\n","str_params = 'cnn_optimizer_Adam_lr_2e-05_epochs_3_bs_6_maxlen_4096'\n","\n","## Paths and filenames\n","absolute_path = '/content/gdrive/My Drive/Projects/SpamNews/'\n","test_file_path = 'Datasets/test_set.csv'\n","predictions_path = 'Datasets/test_set.csv'\n","trained_model = 'TrainedModels/' + str_params + '.keras'\n","feature_col = 'Text'\n","prediction_col = str_params + '_prediction'\n","\n","# Instantiate the CNNPredictions class\n","cnn_predictions = CNNPredictions(max_len, absolute_path, test_file_path, predictions_path, trained_model, feature_col, prediction_col)\n","\n","# Perform predictions\n","cnn_predictions.predict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ApTFf0Ni3R6","executionInfo":{"status":"ok","timestamp":1729918110977,"user_tz":-180,"elapsed":5496,"user":{"displayName":"Konstantinos Roumeliotis","userId":"17264923090131634662"}},"outputId":"05de8a61-bbe5-44b3-9d83-fffcb90596a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","Predictions done\n"]}]}]}